import os

import requests

from bs4 import BeautifulSoup

from urllib.parse import urljoin, urlparse

import markdownify



# --- KONFIGURACE ---

SOURCE_URL = "https://www.headshots.cz/"

OUTPUT_DIR = "migration_data"

IMAGES_DIR = os.path.join(OUTPUT_DIR, "images")



# Hlaviƒçka, abychom nevypadali jako bot (nƒõkter√© servery to blokuj√≠)

HEADERS = {

    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

}



def setup_directories():

    if not os.path.exists(OUTPUT_DIR):

        os.makedirs(OUTPUT_DIR)

    if not os.path.exists(IMAGES_DIR):

        os.makedirs(IMAGES_DIR)

    print(f"üìÇ Slo≈æky p≈ôipraveny: {OUTPUT_DIR}")



def download_image(img_url):

    try:

        response = requests.get(img_url, headers=HEADERS, stream=True)

        if response.status_code == 200:

            # Z√≠sk√°n√≠ n√°zvu souboru z URL

            filename = os.path.basename(urlparse(img_url).path)

            if not filename:

                filename = "image_unknown.jpg"

            

            # O≈°et≈ôen√≠ p≈ô√≠pon a query parametr≈Ø

            if "?" in filename:

                filename = filename.split("?")[0]

            if not os.path.splitext(filename)[1]:

                filename += ".jpg"



            filepath = os.path.join(IMAGES_DIR, filename)

            

            with open(filepath, 'wb') as f:

                for chunk in response.iter_content(1024):

                    f.write(chunk)

            print(f"  ‚úÖ Sta≈æeno: {filename}")

        else:

            print(f"  ‚ùå Chyba stahov√°n√≠ {img_url}: Status {response.status_code}")

    except Exception as e:

        print(f"  ‚ùå Chyba u {img_url}: {e}")



def scrape_site():

    print(f"üöÄ Zaƒç√≠n√°m skenovat: {SOURCE_URL}")

    

    try:

        response = requests.get(SOURCE_URL, headers=HEADERS)

        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')



        # 1. Ulo≈æen√≠ HTML struktury (pro referenci rozlo≈æen√≠)

        html_path = os.path.join(OUTPUT_DIR, "structure.html")

        with open(html_path, "w", encoding="utf-8") as f:

            f.write(soup.prettify())

        print(f"üìÑ HTML struktura ulo≈æena do: {html_path}")



        # 2. Ulo≈æen√≠ obsahu jako Markdown (pro lep≈°√≠ ƒçitelnost AI p≈ôi psan√≠ nov√©ho webu)

        md_content = markdownify.markdownify(str(soup), heading_style="ATX")

        md_path = os.path.join(OUTPUT_DIR, "content.md")

        with open(md_path, "w", encoding="utf-8") as f:

            f.write(md_content)

        print(f"üìù Textov√Ω obsah ulo≈æen do: {md_path}")



        # 3. Stahov√°n√≠ obr√°zk≈Ø

        print("üñºÔ∏è Hled√°m obr√°zky...")

        images = soup.find_all('img')

        img_urls = set()



        for img in images:

            src = img.get('src')

            if src:

                # P≈ôeveden√≠ relativn√≠ URL na absolutn√≠

                full_url = urljoin(SOURCE_URL, src)

                img_urls.add(full_url)



        print(f"Nalezeno {len(img_urls)} unik√°tn√≠ch obr√°zk≈Ø. Stahuji...")

        

        for url in img_urls:

            download_image(url)



    except requests.exceptions.RequestException as e:

        print(f"üõë Kritick√° chyba p≈ôi naƒç√≠t√°n√≠ str√°nky: {e}")



if __name__ == "__main__":

    setup_directories()

    scrape_site()

    print("\n‚ú® Hotovo! Nyn√≠ m≈Ø≈æe≈° slo≈æku 'migration_data' pou≈æ√≠t pro White Rabbit workflow.")


